\documentclass{beamer}
\mode<presentation>

\usepackage{amsmath}
\DeclareMathOperator*{\argmin}{\arg\!\min}

\usetheme{Singapore}
\title{Distributed Dual Averaging in Networks}
\subtitle{Project Presentation}
\author{Maxim Timchenko}
\institute{Electrical and Computer Engineering Department\\Boston University}
\date{EC 719 Statistical Pattern Recognition, Fall 2014}
\begin{document}
	\begin{frame}
		\titlepage
	\end{frame}
	
	\begin{frame}{The Paper}
		Agarwal, Alekh, Martin J. Wainwright, and John C. Duchi. 
		"\href{http://web.stanford.edu/~jduchi/projects/DuchiAgWa10_nips.pdf}{Distributed dual averaging in networks.}" 
		In Advances in Neural Information Processing Systems, pp. 550-558. 2010.		
	\end{frame}
	
	\begin{frame}{Outline}
		\tableofcontents
	\end{frame}
	
	\section{The Problem}
	\begin{frame}{Problem Statement}
		\begin{description}
  			\item[Decentralized optimization]\hfill \\ Optimize a global objective
			formed by a sum of convex functions using local computation
			and local communication over a network of compute nodes.
			\pause
			\item[Theoretical performance] \hfill \\ Provide bounds on algorithm 
			convergence rates as function of network size and topology.
		\end{description}
	\end{frame}
	
	\section{Motivation}
	\begin{frame}{Motivation: Big Data}
		Classical ML: minimize a loss function over a dataset.\\
		Interesting datasets grow in size faster than innovations in 
		storage capacity of a single computer.
		\begin{itemize}
			\item Google Maps dataset in 2012: 20 PB (20,500 TB)\footnote{http://mashable.com/2012/08/22/google-maps-facts/}
			\item Facebook dataset in 2010: 20 PB, in 2011: 30 PB\footnote{https://www.facebook.com/notes/paul-yang/moving-an-elephant-large-scale-hadoop-data-migration-at-facebook/10150246275318920}
		\end{itemize}
		Non-ML distributed convex minimization: 
		multi-agent coordination, estimation in sensor networks,
		packet routing...
	\end{frame}	

	\begin{frame}{Motivation: Distributed Computation Constraints}
		In datacenter environments, supercomputers, and ad-hoc
		distributed networks, available bandwidth of communication 
		is in inverse relationship to distance.
		\begin{quote}Traditionally, inter-cluster connectivity is oversubscribed, with much less bandwidth available between the clusters than within them. This assumes and then dictates that most intra-application communications occur inside the cluster."\footnote{https://code.facebook.com/posts/360346274145943/introducing-data-center-fabric-the-next-generation-facebook-data-center-network/}\end{quote}
		
		Modelling the network as a graph with nearby connections as
		edges is convenient.
	\end{frame}
	
	\section{Related Work}
	\begin{frame}{Related Work}
		\begin{itemize}
			\item N. Tsitsiklis, D. P. Bertsekas, and M. Athans. 1986.
Distributed asynchronous deterministic and stochastic
gradient optimization algorithms. \emph{Uses shared memory.}
			\item D. P. Bertsekas, J. N. Tsitsiklis. Parallel and Distributed Computation: Numerical Methods. 1989.
			\item A. Nedic and A. Ozdaglar. Distributed subgradient methods for multi-agent optimization. 2009.\\ 
			\emph{Each agent has its own objective function.}
			\item Y. Nesterov. Primal-dual subgradient methods for convex problems. 2009. \\
			\emph{Non-distributed version of the proposed algorithm.}
		\end{itemize}
	\end{frame}
	
	\begin{frame}{Primal-dual subgradient methods}
		\begin{itemize}
			\item Objective function $f(x)$ is non-smooth and not necessarily differentiable.
			\item Classic: $\min_x\{f(x):x \in \mathbb{R}^n\} : x_{k+1} = x_k - \alpha_k g_k$
			\item Convergence and reach: $\alpha_k \rightarrow 0, \sum_k \alpha_k = \infty.$	
			\item Lower linear model of the objective function:
			\[ l_k(x) \overset{def}{=} \sum_{i=0}^k \alpha_i(f(x_i) + \langle g_i, x-x_i \rangle) / \sum_{i=0}^k  \alpha_i \]
			\item New subgradients are added with \emph{decreasing} weights, needed for convergence
				of primal sequence $\{x_k\}$; but they are more important.
		\end{itemize}
	\end{frame}
	
	\begin{frame}{Primal-dual subgradient methods}
		\begin{itemize}
			\item Insight: maintain two different sequences of parameters.
			\item One is responsible for the primal space process and has a vanishing sequence of steps.
			\item The other is in the dual space of linear function and has a non-decreasing sequence of 
				weights to prioritize newer subgradients.
		\end{itemize}
	\end{frame}	
	
	\section{Contribution}
	\begin{frame}{The Paper's Experimental Results}
		\begin{itemize}
			\item	The paper's simulations are done using
				synthetic SVM classification problems with hinge loss and
				\[\mathcal{X} = \{x \in \mathbb{R}^d \mid \|x\|_2 \leq 5\}.\]
			\item Performance is evaluated for different network sizes (n=100, 225, 400, 625, 900)
				and topology (single cycle, 2-D grid, bounded degree expander).
			\item ``We have observed qualitatively similar behavior for other problem classes''.
		\end{itemize}
	\end{frame}
	
	\subsection{Methodology}
	\begin{frame}{Our Experiments}
		\begin{itemize}
			\item Choose another class of optimization problems (non-SVM).
			\item Compare convergence rate of non-distributed primal-dual subgradient method
				to the proposed distributed method.
			\item Plot convergence rate vs. network size and parameters for a random geometric
				graph (mentioned in the paper but not evaluated).\footnote{A random geometric
				graph can model an ad-hoc wireless sensor network with limited communication range.}
			\item Try the approach on a real dataset.				
		\end{itemize}
	\end{frame}
	
	\begin{frame}{Experiments}
		\begin{itemize}
			\item Implemented Dual Averaging for a 2D logistic regression: simple to understand,
				fast to simulate, easy to generate unlimited amount of test data.
			\item $f(\theta) = \min-\sum_{i=1}^{m} y^{(i)}\log h_\theta(x^{(i)}) + (1- y^{(i)})\log (1 - h_\theta(x^{(i)})$
			\item $z(t+1) = z(t) - g(t) = z(t) - [\mathbf{x}(\mathbf{y} - h_\theta(\mathbf{x}))']$
			\item $\alpha(t) = 1/t, \psi(\theta) = \frac{1}{2}\| \theta \|^2$
			\item $\theta(t+1) = \Pi^\psi(-z(t+1), \alpha(t))$
			\item $ \Pi^\psi(z, \alpha) = \argmin_{\theta}(\langle z, \theta \rangle + \frac{1}{\alpha}\psi(\theta))$
			\item \ldots a bit of algebra \ldots
			\item $\theta(t+1) = -\frac{\alpha}{2}z$			
		\end{itemize}
	\end{frame}	
	
	\begin{frame}{Single-node Dual Subgradient Method}
		 \includegraphics[width=\textwidth]{comp-vs-nr.png}\\
		 Compared to Newton-Raphson, convergence to same tolerance is much slower (258 iterations vs. 31)
	\end{frame}
	
	\begin{frame}{Single-node vs. 9-node Cycle Distributed DA}
		\centerline{\includegraphics[width=0.75\textwidth]{singlenode-vs-dist.png}}
		Iterations required: single-node = 261, distributed = 672.
	\end{frame}
	
	\begin{frame}{Convergence of a 16-node Cycle vs. 16-node Grid}
		\centerline{\includegraphics[width=\textwidth]{grid-vs-cycle-ll-per-node.png}}
		As connectivity improves, convergence speeds up: cycle graph iterations = 744, grid graph iterations = 383.
	\end{frame}
	
	\begin{frame}{Random Geometric Graph: Density}
		\centerline{\includegraphics[width=\textwidth]{density_vs_conv.png}}
		As expected, strong connectivity improves convergence speed.
	\end{frame}	

	\begin{frame}{Random Geometric Graph: Size (10-80 nodes)}
		\centerline{\includegraphics[width=\textwidth]{size_vs_conv.png}}
		The opposite result was expected; it looks like the effects of stronger connectivity
		outweigh the (quadratic) growth of area covered by the nodes of the graph.
	\end{frame}	
	
	\begin{frame}{Kernel SVM experiment}
		\begin{itemize}
			\item Implement a kernelized SVM dual descent algorithm (``optimal soft margin linear classifier without offset'' 
			from HW2) on MNIST 4/9 8x8 dataset.
			\item Use kernel $@(a,b)(a' * b + 1)$ - fast but shows decent benchmark results with SMO.
			\item Inhomogenous polynomial kernel works well in a no-constant-term framework.
			\item The optimization problem is the SVM dual:\[f(\alpha) = \frac{1}{2}\sum_{i,j}\alpha_i\alpha_jy^{(i)}y^{(j)}K(x^{(i)}, x^{(j)}) + \sum_i\alpha_i\]
			\item Optimal solution for $z$:\[z_i = \frac{1 - \frac{1}{2}\sum_{i \not = j}\alpha_jy^{(i)}y^{(j)}K(x^{(i)}, x^{(j)})}{K(x^{(i)}, x^{(i)})}\]
		\end{itemize}
	\end{frame}
				
	\begin{frame}{Kernel SVM results}
		\begin{itemize}
			\item After 500 iterations with $C = 1000$, $num\_sv = 1107$ and $test\_error = 0.064$.
			\item SMO result on the same kernel and C (normal / constant term framework): 0.045 with 234 SV.
			\item Conclusion: likely to be an implementation bug (the projection is now a constrained
				minimization problem, unlike logistic regression, since $0 \leq \alpha_i \leq \frac{C}{n})$.
			\item Dual averaging is only efficient when the projection from dual to primal space $ \Pi$ can
				be efficiently computed.
		\end{itemize}
	\end{frame}
	
	\section{Conclusions}
	\begin{frame}{Conclusions: Tunable Parameters}
		\begin{itemize}
			\item	The details of the doubly stochastic graph matrix (for example, how much weight to 
			assign to the local node?) influence convergence speed significantly but are not discussed.
			\item Step choice $\alpha(t)$ and convergence rate: with the suggested formula
			\[\alpha(t) = \frac{R\sqrt{1-\sigma_2(P)}}{4L\sqrt{t}}\]
			the convergence was extremely slow. Therefore we were unable to test the convergence
			corollaries for different graph types.
		\end{itemize}			
	\end{frame}
	
	\begin{frame}{Conclusions: SVM}
		\begin{itemize}
			\item	Computing the projection is a more complicated task with a constrained problem and
				is likely an iterative optimization run in its own right.
			\item The goal of distributed computation in this case is not clear (what is the meaning of
				averaging of $\alpha_i$'s?)
		\end{itemize}			
	\end{frame}	
	
	\begin{frame}{Questions from the Audience}
		The Matlab code used to produce the output for this presentation and the presentation itself
		can be found on GitHub:
		
		\begin{block}{}
		\url{https://github.com/maxvt/science/tree/master/ec719_machine_learning/distributed_dual_averaging}
		\end{block}
	\end{frame}
\end{document}